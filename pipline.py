import argparse
import warnings
import os
import torch
import numpy as np
from pytorch_HNECV import getdataloader,train_VAE
from RandomWalk import GeneratorMetaPath_by_randomwalk
from RandomWalk_Yelp import GeneratorMetaPath_by_randomwalk_yelp
from utils import Dataset,build_fusion_mat,select_embeddingBylabel
from Node_classifier import preprocess_for_knn
from Node_clustering import preprocess_for_kmeans
from TSNE import preprocess_for_tsne


def start(args):
    dataset = Dataset()
    if args.dataset == 'dblp':
        dataset.preprocess_dblp()
        init_class = GeneratorMetaPath_by_randomwalk()
        # Initial data set path
        dirpath = "dataset/DBLP/reindex_dblp/"
        # output path
        output_path = "dataset/DBLP/dblp_metapath_by_rw/"
        # Number of rows generated by a single start node
        numwalks = 10
        # walk length
        walklength = 80
        # Save the meta path sequence file path
        outputfile_APA = output_path + "APA_metapath_DBLP_r" + str(
            numwalks) + "_c" + str(walklength) + ".txt"
        outputfile_APCPA = output_path + "APCPA_metapath_DBLP_r" + str(
            numwalks) + "_c" + str(walklength) + ".txt"
        outputfile_APTPA = output_path + "APTPA_metapath_DBLP_r" + str(
            numwalks) + "_c" + str(walklength) + ".txt"
        init_class.build_adj_dict(dirpath, 'paper_author_new.txt', init_class.paper_author, init_class.author_paper)
        init_class.build_adj_dict(dirpath, 'paper_conf_new.txt', init_class.paper_conf, init_class.conf_paper)
        init_class.build_adj_dict(dirpath, 'paper_term_new.txt', init_class.paper_term, init_class.term_paper)
        print("start random walk.....")
        init_class.parallel_random_walk(init_class.random_walk_by_APA, outputfile_APA, numwalks, walklength)
        init_class.parallel_random_walk(init_class.random_walk_by_APCPA, outputfile_APCPA, numwalks,
                                        walklength)
        init_class.parallel_random_walk(init_class.random_walk_by_APTPA, outputfile_APTPA, numwalks,
                                        walklength)
        dataset = Dataset()
        dataset.parallel_process(output_path)
        graph_mat_path = 'dataset/DBLP/dblp_graph_mat/'
        dataset.build_graph_from_rwfile_mat(graph_mat_path, output_path)
        build_fusion_mat(graph_mat_path,args.dataset)
    if args.dataset == 'aminer':
        dataset.preprocess_aminer()
        init_class = GeneratorMetaPath_by_randomwalk()
        # Initial data set path
        dirpath = "dataset/AMiner/reindex_aminer/"
        # output path
        output_path = "dataset/AMiner/aminer_metapath_by_rw/"
        # Number of rows generated by a single start node
        numwalks = 10
        # walk length
        walklength = 80
        # Save the meta path sequence file path
        outputfile_APA = output_path + "APA_metapath_Aminer_r" + str(
            numwalks) + "_c" + str(walklength) + ".txt"
        outputfile_APCPA = output_path + "APCPA_metapath_Aminer_r" + str(
            numwalks) + "_c" + str(walklength) + ".txt"
        init_class.build_adj_dict(dirpath, 'paper_author_new.txt', init_class.paper_author, init_class.author_paper)
        init_class.build_adj_dict(dirpath, 'paper_conf_new.txt', init_class.paper_conf, init_class.conf_paper)
        print("start random walk.....")
        init_class.parallel_random_walk(init_class.random_walk_by_APA, outputfile_APA, numwalks, walklength)
        init_class.parallel_random_walk(init_class.random_walk_by_APCPA, outputfile_APCPA, numwalks,
                                        walklength)
        dataset = Dataset()
        dataset.parallel_process(output_path)
        graph_mat_path = 'dataset/AMiner/aminer_graph_mat/'
        dataset.build_graph_from_rwfile_mat(graph_mat_path, output_path)
        build_fusion_mat(graph_mat_path, args.dataset)
    if args.dataset == 'yelp':
        dataset.preprocess_yelp()
        init_class = GeneratorMetaPath_by_randomwalk_yelp()
        # Initial data set path
        dirpath = "dataset/Yelp/sourcedata/"
        # output path
        output_path = "dataset/Yelp/yelp_metapath_by_rw/"
        # Number of rows generated by a single start node
        numwalks = 10
        # walk length
        walklength = 80
        # Save the meta path sequence file path
        outputfile_BSB = output_path+"BSB_metapath_Yelp_r" + str(numwalks) + "_c" + str(
            walklength) + ".txt"
        outputfile_BStB = output_path+"BStB_metapath_Yelp_r" + str(numwalks) + "_c" + str(
            walklength) + ".txt"
        outputfile_BUB = output_path+"BUB_metapath_Yelp_r" + str(numwalks) + "_c" + str(
            walklength) + ".txt"
        init_class.build_adj_dict(dirpath, 'business_service.txt', init_class.business_service,
                                  init_class.service_business)
        init_class.build_adj_dict(dirpath, 'business_stars.txt', init_class.business_stars, init_class.stars_business)
        init_class.build_adj_dict(dirpath, 'business_user.txt', init_class.business_user, init_class.user_business)
        print("start random walk.....")
        init_class.parallel_random_walk(init_class.random_walk_by_BSB, outputfile_BSB, numwalks, walklength)
        init_class.parallel_random_walk(init_class.random_walk_by_BStB, outputfile_BStB, numwalks, walklength)
        init_class.parallel_random_walk(init_class.random_walk_by_BUB, outputfile_BUB, numwalks, walklength)
        dataset = Dataset()
        dataset.parallel_process(output_path)
        graph_mat_path = 'dataset/Yelp/yelp_graph_mat/'
        dataset.build_graph_from_rwfile_mat(graph_mat_path, output_path)
        build_fusion_mat(graph_mat_path, args.dataset)

    N, data_loader, data_loader2 = getdataloader(args.input_path, args.batch,device)
    print(args)
    z = train_VAE(N, args.lr, args.n_epochs, args.pretrain_epochs, args.batch, args.M,
                  args.loss_path, args.model_path,
                  args.pretrain_path, args.beta, data_loader, data_loader2,device)
    if args.isValidate == True:
        selectedEmbedding, selectedlabel = select_embeddingBylabel(z, args.label_path)
        if args.savembedding == True:
            np.savetxt(args.embedding_path, selectedEmbedding, delimiter=',', fmt='%.07f')
        preprocess_for_knn(selectedEmbedding, selectedlabel)
        preprocess_for_kmeans(selectedEmbedding, selectedlabel, args.M)
        if args.dataset == 'dblp':
            preprocess_for_tsne(selectedEmbedding, selectedlabel)
    else:
        np.savetxt(args.embedding_path, z, delimiter=',', fmt='%.07f')

if __name__ == '__main__':
    warnings.filterwarnings("ignore")
    os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    parser = argparse.ArgumentParser(
        description='pipline',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--dataset', type=str, default='dblp')
    parser.add_argument('--n_epochs', type=int, default=100)
    parser.add_argument('--pretrain_epochs', type=int, default=30)
    parser.add_argument('--batch', type=int, default=32)
    parser.add_argument('--M', type=int, default=4)
    parser.add_argument('--beta', type=int, default=2)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--dim', type=int, default=128)
    parser.add_argument('--pretrain_path', type=str)
    parser.add_argument('--input_path', type=str)
    parser.add_argument('--embedding_path', type=str)
    parser.add_argument('--loss_path', type=str)
    parser.add_argument('--model_path', type=str)
    parser.add_argument('--label_path', type=str)
    parser.add_argument('--isValidate', type=bool, default=True)
    parser.add_argument('--savembedding', type=bool, default=True)
    args = parser.parse_args()
    args.cuda = torch.cuda.is_available()
    print("use cuda: {}".format(args.cuda))
    device = torch.device("cuda" if args.cuda else "cpu")
    if args.dataset == 'dblp':
        args.M = 4
        args.lr = 0.0001
        args.batch = 128
        args.beta = 2
        args.n_epochs = 100
        args.pretrain_epochs = 30
        args.pretrain_path = "./pretrain_ae.pkl"
        args.input_path = "dataset/DBLP/test/Single_DBLP_mat.mat"
        args.embedding_path = "modelResult/DBLP/dblp_embedding.txt"
        args.loss_path = "loss.txt"
        args.model_path = "modelResult/DBLP/dblp.pkl"
        args.label_path = "dataset/DBLP/reindex_dblp/author_label_new.txt"
        args.isValidate = True
        args.savembedding = True

    if args.dataset == 'aminer':
        args.M = 8
        args.lr = 0.0001
        args.batch = 128
        args.beta = 5
        args.n_epochs = 100
        args.pretrain_epochs = 30
        args.pretrain_path = "./pretrain_ae.pkl"
        args.input_path = "dataset/AMiner/test/Single_Aminer_mat.mat"
        args.embedding_path = "modelResult/AMiner/aminer_embedding.txt"
        args.loss_path = "loss.txt"
        args.model_path = "modelResult/AMiner/aminer.pkl"
        args.label_path = "dataset/AMiner/reindex_aminer/author_label_new.txt"
        args.isValidate = True
        args.savembedding = True

    if args.dataset == 'yelp':
        args.M = 3
        args.lr = 0.0003
        args.batch = 32
        args.beta = 15
        args.n_epochs = 100
        args.pretrain_epochs = 50
        args.pretrain_path = "./pretrain_ae.pkl"
        args.input_path = "dataset/Yelp/test/Single_Yelp_mat.mat"
        args.embedding_path = "modelResult/Yelp/yelp_embedding.txt"
        args.loss_path = "loss.txt"
        args.model_path = "modelResult/Yelp/yelp.pkl"
        args.label_path = "dataset/Yelp/entity/business_label.txt"
        args.isValidate = True
        args.savembedding = True
    start(args)
