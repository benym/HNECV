
import csv
import multiprocessing as mp
import os
import random
import time

import numpy as np
from tqdm import tqdm



class GeneratorMetaPath_by_randomwalk:
    def __init__(self):
        self.paper_author = dict()
        self.author_paper = dict()
        self.paper_conf = dict()
        self.conf_paper = dict()
        self.paper_term = dict()
        self.term_paper = dict()

    def build_adj_dict(self, dirpath, filename, entityA_entityB_dict, entityB_entityA_dict):
        """
        Construct a dict between entityA_id and entityB_id (bi-directional build)
        :param dirpath: Initial data set path
        :param filename: The file name of the dict whose bidirectional id needs to be constructed
        :param entityA_entityB_dict: Corresponding to the dict of A_B in self
        :param entityB_entityA_dict: Corresponding to the dict of B_A in self
        """
        with open(dirpath + filename) as file:
            for line in file:
                line_words = line.strip().split("\t")
                if len(line_words) == 2:
                    A, B = line_words[0], line_words[1]
                    if A not in entityA_entityB_dict:
                        entityA_entityB_dict[A] = []
                    entityA_entityB_dict[A].append(B)
                    if B not in entityB_entityA_dict:
                        entityB_entityA_dict[B] = []
                    entityB_entityA_dict[B].append(A)

    def parallel_callback(self, file_data):
        """
        Parallel callback, sub-process data centralization, collective writing
        :param file_data: dict type, key is the file path, value corresponds to the sequence of files to be written
        """
        output_dir = list(file_data.keys())[0]
        output_file = csv.writer(open(output_dir, 'a', encoding='utf-8', newline=""), delimiter=' ')
        for i in file_data[output_dir]:
            output_file.writerow(i)

    def parallel_random_walk(self, method, outfilename, numwalks, walklength):
        """
        Parallelized random walk
        :param method: Random walk method corresponding to metapath
        :param outfilename: output file name
        :param numwalks: Number of rows generated by a single start node
        :param walklength: walk length
        """
        start = time.time()
        global file_data
        file_data = dict()
        file_data[outfilename] = list()
        if os.path.exists(outfilename):
            with open(outfilename, 'r+') as file:
                file.truncate()
        author = list(self.author_paper.keys())
        author_subset = np.array_split(author, 4)
        pool = mp.Pool(mp.cpu_count())
        for s_author in author_subset:
            pool.apply_async(method, args=(s_author, outfilename, file_data, numwalks, walklength),
                             callback=self.parallel_callback)
        pool.close()
        pool.join()
        # method(author,outfilename,file_data,numwalks,walklength)
        end = time.time()
        print("cost:{} mins".format((end - start) / 60))

    def random_walk_by_APA(self, sub_authorSet, outfilename, file_data, numwalks, walklength):
        """
        Random walk with APA as metapath
        :param sub_authorSet: Author subset sent by a single process
        :param outfilename: output file name
        :param file_data: Global write file dict
        :param numwalks: Number of rows generated by a single start node
        :param walklength: walk length
        :return: Generated metapath sequence data
        """
        for author in tqdm(sub_authorSet):
            for i in range(numwalks):
                single_node_seq = []
                single_node_seq.append("A" + author)
                for j in range(walklength):
                    previous_author = single_node_seq[-1][1:]
                    papers = self.author_paper[previous_author]
                    next_paper = random.choice(papers)
                    # single_node_seq.append("P" + next_paper)
                    authors = self.paper_author[next_paper]
                    next_author = random.choice(authors)
                    single_node_seq.append("A" + str(next_author))
                file_data[outfilename].append(single_node_seq)
        return file_data

    def random_walk_by_APCPA(self, sub_authorSet, outfilename, file_data, numwalks, walklength):
        """
        Random walk with APCPA as metapath
        :param sub_authorSet: Author subset sent by a single process
        :param outfilename: output file name
        :param file_data: Global write file dict
        :param numwalks: Number of rows generated by a single start node
        :param walklength: walk length
        :return: Generated metapath sequence data
        """
        for author in tqdm(sub_authorSet):
            for i in range(numwalks):
                single_node_seq = []
                single_node_seq.append("A" + author)
                for j in range(walklength):
                    previous_author = single_node_seq[-1][1:]
                    papers = self.author_paper[previous_author]
                    next_paper = random.choice(papers)
                    # single_node_seq.append("P" + next_paper)
                    confs = self.paper_conf[next_paper]
                    next_conf = random.choice(confs)
                    # single_node_seq.append("C" + next_conf)
                    re_papers = self.conf_paper[next_conf]
                    re_next_paper = random.choice(re_papers)
                    # single_node_seq.append("P" + re_next_paper)
                    authors = self.paper_author[re_next_paper]
                    next_author = random.choice(authors)
                    single_node_seq.append("A" + str(next_author))
                file_data[outfilename].append(single_node_seq)
        return file_data

    def random_walk_by_APTPA(self, sub_authorSet, outfilename, file_data, numwalks, walklength):
        """
        Random walk with APTPA as metapath
        :param sub_authorSet: Author subset sent by a single process
        :param outfilename: output file name
        :param file_data: Global write file dict
        :param numwalks: Number of rows generated by a single start node
        :param walklength: walk length
        :return: Generated metapath sequence data
        """
        for author in tqdm(sub_authorSet):
            for i in range(numwalks):
                single_node_seq = []
                single_node_seq.append("A" + author)
                for j in range(walklength):
                    previous_author = single_node_seq[-1][1:]
                    papers = self.author_paper[previous_author]
                    next_paper = random.choice(papers)
                    # single_node_seq.append("P" + next_paper)
                    terms = self.paper_term[next_paper]
                    next_term = random.choice(terms)
                    # single_node_seq.append("T" + next_term)
                    re_papers = self.term_paper[next_term]
                    re_next_paper = random.choice(re_papers)
                    # single_node_seq.append("P" + re_next_paper)
                    authors = self.paper_author[re_next_paper]
                    next_author = random.choice(authors)
                    single_node_seq.append("A" + str(next_author))
                file_data[outfilename].append(single_node_seq)
        return file_data

